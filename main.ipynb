{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:33.428897Z",
     "start_time": "2024-01-25T11:32:20.721064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (1.26.3)\r\n",
      "Requirement already satisfied: jax[cpu] in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (0.4.23)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from jax[cpu]) (0.3.2)\r\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from jax[cpu]) (1.26.3)\r\n",
      "Requirement already satisfied: opt-einsum in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from jax[cpu]) (3.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.9 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from jax[cpu]) (1.11.4)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from jax[cpu]) (7.0.1)\r\n",
      "Requirement already satisfied: jaxlib==0.4.23 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from jax[cpu]) (0.4.23)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from importlib-metadata>=4.6->jax[cpu]) (3.17.0)\r\n",
      "Requirement already satisfied: torch in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from torch) (2023.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: openai==0.28.1 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (0.28.1)\r\n",
      "Requirement already satisfied: requests>=2.20 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from openai==0.28.1) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from openai==0.28.1) (4.66.1)\r\n",
      "Requirement already satisfied: aiohttp in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from openai==0.28.1) (3.9.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from requests>=2.20->openai==0.28.1) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from requests>=2.20->openai==0.28.1) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from requests>=2.20->openai==0.28.1) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from requests>=2.20->openai==0.28.1) (2023.11.17)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (23.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (1.9.4)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from aiohttp->openai==0.28.1) (4.0.3)\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0mRequirement already satisfied: tiktoken in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (0.5.2)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from tiktoken) (2023.12.25)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from tiktoken) (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\r\n",
      "Requirement already satisfied: tqdm in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (4.66.1)\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0mRequirement already satisfied: matplotlib in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (3.8.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (4.47.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (1.4.5)\r\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (1.26.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (23.1)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (10.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from matplotlib) (6.1.1)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/shubhagarwal/anaconda3/envs/llmtime/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install -U jax[cpu] # we don't need GPU for jax\n",
    "!pip install torch\n",
    "!pip install openai==0.28.1\n",
    "!pip install tiktoken\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install \"pandas<2.0.0\"\n",
    "!pip install darts\n",
    "!pip install gpytorch\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install multiprocess\n",
    "!pip install SentencePiece\n",
    "!pip install accelerate\n",
    "!pip install gdown"
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:33.480720Z",
     "start_time": "2024-01-25T11:32:33.431989Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install huggingface-cli\n",
    "!huggingface-cli login"
   ],
   "id": "b51269032ee71963"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:35:13.655350Z",
     "start_time": "2024-01-25T11:35:13.536691Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class SerializerSettings:\n",
    "    \"\"\"\n",
    "    Settings for serialization of numbers.\n",
    "\n",
    "    Attributes:\n",
    "    - base (int): The base for number representation.\n",
    "    - prec (int): The precision after the 'decimal' point in the base representation.\n",
    "    - signed (bool): If True, allows negative numbers. Default is False.\n",
    "    - fixed_length (bool): If True, ensures fixed length of serialized string. Default is False.\n",
    "    - max_val (float): Maximum absolute value of number for serialization.\n",
    "    - time_sep (str): Separator for different time steps.\n",
    "    - bit_sep (str): Separator for individual digits.\n",
    "    - plus_sign (str): String representation for positive sign.\n",
    "    - minus_sign (str): String representation for negative sign.\n",
    "    - half_bin_correction (bool): If True, applies half bin correction during deserialization. Default is True.\n",
    "    - decimal_point (str): String representation for the decimal point.\n",
    "    \"\"\"\n",
    "    base: int = 10\n",
    "    prec: int = 3\n",
    "    signed: bool = True\n",
    "    fixed_length: bool = False\n",
    "    max_val: float = 1e7\n",
    "    time_sep: str = ' ,'\n",
    "    bit_sep: str = ' '\n",
    "    plus_sign: str = ''\n",
    "    minus_sign: str = ' -'\n",
    "    half_bin_correction: bool = True\n",
    "    decimal_point: str = ''\n",
    "    missing_str: str = ' Nan'"
   ],
   "id": "62e8f6e27b6ae860",
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:41.811021Z",
     "start_time": "2024-01-25T11:32:41.774190Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def make_validation_dataset(train, n_val, val_length):\n",
    "    \"\"\"Partition the training set into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        train (list): List of time series data for training.\n",
    "        n_val (int): Number of validation samples.\n",
    "        val_length (int): Length of each validation sample.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Lists of training data without validation, validation data, and number of validation samples.\n",
    "    \"\"\"\n",
    "    assert isinstance(train, list), 'Train should be a list of series'\n",
    "\n",
    "    train_minus_val_list, val_list = [], []\n",
    "    if n_val is None:\n",
    "        n_val = len(train)\n",
    "    for train_series in train[:n_val]:\n",
    "        train_len = max(len(train_series) - val_length, 1)\n",
    "        train_minus_val, val = train_series[:train_len], train_series[train_len:]\n",
    "        print(f'Train length: {len(train_minus_val)}, Val length: {len(val)}')\n",
    "        train_minus_val_list.append(train_minus_val)\n",
    "        val_list.append(val)\n",
    "\n",
    "    return train_minus_val_list, val_list, n_val"
   ],
   "id": "1465c9fd0370dc07",
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:42.049877Z",
     "start_time": "2024-01-25T11:32:42.007321Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_hyper(hyper, train_minus_val, val, get_predictions_fn):\n",
    "    \"\"\"Evaluate a set of hyperparameters on the validation set.\n",
    "\n",
    "    Args:\n",
    "        hyper (dict): Dictionary of hyperparameters to evaluate.\n",
    "        train_minus_val (list): List of training samples minus validation samples.\n",
    "        val (list): List of validation samples.\n",
    "        get_predictions_fn (callable): Function to get predictions.\n",
    "\n",
    "    Returns:\n",
    "        float: NLL/D value for the given hyperparameters, averaged over each series.\n",
    "    \"\"\"\n",
    "    assert isinstance(train_minus_val, list) and isinstance(val, list), 'Train minus val and val should be lists of series'\n",
    "    return get_predictions_fn(train_minus_val, val, **hyper, num_samples=0)['NLL/D']"
   ],
   "id": "88ad5d8a9220ed4b",
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:42.268453Z",
     "start_time": "2024-01-25T11:32:42.226668Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dataclasses import is_dataclass\n",
    "def convert_to_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_dict(elem) for elem in obj]\n",
    "    elif is_dataclass(obj):\n",
    "        return convert_to_dict(obj.__dict__)\n",
    "    else:\n",
    "        return obj"
   ],
   "id": "ecb1c788c73daad0",
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:42.521665Z",
     "start_time": "2024-01-25T11:32:42.486042Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def flatten(d, parent_key='', sep='/'):\n",
    "    \"\"\"An invertible dictionary flattening operation that does not clobber objs\"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict) and v: # non-empty dict\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ],
   "id": "5ebc2bc1f91127a1",
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:42.748907Z",
     "start_time": "2024-01-25T11:32:42.709665Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def unflatten(d,sep='/'):\n",
    "    \"\"\"Take a dictionary with keys {'k1/k2/k3':v} to {'k1':{'k2':{'k3':v}}}\n",
    "        as outputted by flatten \"\"\"\n",
    "    out_dict={}\n",
    "    for k,v in d.items():\n",
    "        if isinstance(k,str):\n",
    "            keys = k.split(sep)\n",
    "            dict_to_modify = out_dict\n",
    "            for partial_key in keys[:-1]:\n",
    "                try: dict_to_modify = dict_to_modify[partial_key]\n",
    "                except KeyError:\n",
    "                    dict_to_modify[partial_key] = {}\n",
    "                    dict_to_modify = dict_to_modify[partial_key]\n",
    "                # Base level reached\n",
    "            if keys[-1] in dict_to_modify:\n",
    "                dict_to_modify[keys[-1]].update(v)\n",
    "            else:\n",
    "                dict_to_modify[keys[-1]] = v\n",
    "        else: out_dict[k]=v\n",
    "    return out_dict"
   ],
   "id": "98cd4e90fb569048",
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:42.937728Z",
     "start_time": "2024-01-25T11:32:42.901297Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "class FixedNumpySeed(object):\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "    def __enter__(self):\n",
    "        self.np_rng_state = np.random.get_state()\n",
    "        np.random.seed(self.seed)\n",
    "        self.rand_rng_state = random.getstate()\n",
    "        random.seed(self.seed)\n",
    "    def __exit__(self, *args):\n",
    "        np.random.set_state(self.np_rng_state)\n",
    "        random.setstate(self.rand_rng_state)"
   ],
   "id": "99d1ea313b212429",
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:43.179331Z",
     "start_time": "2024-01-25T11:32:43.140520Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def _sample_config(config_spec,cfg_all):\n",
    "    cfg = {}\n",
    "    more_work = False\n",
    "    for k,v in config_spec.items():\n",
    "        if isinstance(v,dict):\n",
    "            new_dict,extra_work = _sample_config(v,cfg_all)\n",
    "            cfg[k] = new_dict\n",
    "            more_work |= extra_work\n",
    "        elif isinstance(v,Iterable) and not isinstance(v,(str,bytes,dict,tuple)):\n",
    "            cfg[k] = random.choice(v)\n",
    "        elif callable(v) and v.__name__ == \"<lambda>\":\n",
    "            try:cfg[k] = v(cfg_all)\n",
    "            except (KeyError, LookupError,Exception):\n",
    "                cfg[k] = v # is used isntead of the variable it returns\n",
    "                more_work = True\n",
    "        else: cfg[k] = v\n",
    "    return cfg, more_work"
   ],
   "id": "d7a76233603c97df",
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:43.375346Z",
     "start_time": "2024-01-25T11:32:43.339455Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NoGetItLambdaDict(dict):\n",
    "    \"\"\" Regular dict, but refuses to __getitem__ pretending\n",
    "        the element is not there and throws a KeyError\n",
    "        if the value is a non string iterable or a lambda \"\"\"\n",
    "    def __init__(self,d={}):\n",
    "        super().__init__()\n",
    "        for k,v in d.items():\n",
    "            if isinstance(v,dict):\n",
    "                self[k] = NoGetItLambdaDict(v)\n",
    "            else:\n",
    "                self[k] = v\n",
    "    def __getitem__(self, key):\n",
    "        value = super().__getitem__(key)\n",
    "        if callable(value) and value.__name__ == \"<lambda>\":\n",
    "            raise LookupError(\"You shouldn't try to retrieve lambda {} from this dict\".format(value))\n",
    "        if isinstance(value,Iterable) and not isinstance(value,(str,bytes,dict,tuple)):\n",
    "            raise LookupError(\"You shouldn't try to retrieve iterable {} from this dict\".format(value))\n",
    "        return value\n",
    "        \n",
    "    # pop = __readonly__\n",
    "    # popitem = __readonly__"
   ],
   "id": "683d6accac9e380d",
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:43.641098Z",
     "start_time": "2024-01-25T11:32:43.599335Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def sample_config(config_spec):\n",
    "    \"\"\" Generates configs from the config spec.\n",
    "        It will apply lambdas that depend on the config and sample from any\n",
    "        iterables, make sure that no elements in the generated config are meant to \n",
    "        be iterable or lambdas, strings are allowed.\"\"\"\n",
    "    cfg_all = config_spec\n",
    "    more_work=True\n",
    "    i=0\n",
    "    while more_work:\n",
    "        cfg_all, more_work = _sample_config(cfg_all,NoGetItLambdaDict(cfg_all))\n",
    "        i+=1\n",
    "        if i>10: \n",
    "            raise RecursionError(\"config dependency unresolvable with {}\".format(cfg_all))\n",
    "    out = defaultdict(dict)\n",
    "    out.update(cfg_all)\n",
    "    return out"
   ],
   "id": "6fbc1a4aafe9c6ec",
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:43.861314Z",
     "start_time": "2024-01-25T11:32:43.816699Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import itertools, functools\n",
    "import operator\n",
    "\n",
    "class grid_iter(object):\n",
    "    \"\"\" Defines a length which corresponds to one full pass through the grid\n",
    "        defined by grid variables in config_spec, but the iterator will continue iterating\n",
    "        past that by repeating over the grid variables\"\"\"\n",
    "    def __init__(self,config_spec,num_elements=-1,shuffle=True):\n",
    "        self.cfg_flat = flatten(config_spec)\n",
    "        is_grid_iterable = lambda v: (isinstance(v,Iterable) and not isinstance(v,(str,bytes,dict,tuple)))\n",
    "        iterables = sorted({k:v for k,v in self.cfg_flat.items() if is_grid_iterable(v)}.items())\n",
    "        if iterables: self.iter_keys,self.iter_vals = zip(*iterables)\n",
    "        else: self.iter_keys,self.iter_vals = [],[[]]\n",
    "        self.vals = list(itertools.product(*self.iter_vals))\n",
    "        if shuffle:\n",
    "            with FixedNumpySeed(0): random.shuffle(self.vals)\n",
    "        self.num_elements = num_elements if num_elements>=0 else (-1*num_elements)*len(self)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i=0\n",
    "        self.vals_iter = iter(self.vals)\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        self.i+=1\n",
    "        if self.i > self.num_elements: raise StopIteration\n",
    "        if not self.vals: v = []\n",
    "        else:\n",
    "            try: v = next(self.vals_iter)\n",
    "            except StopIteration:\n",
    "                self.vals_iter = iter(self.vals)\n",
    "                v = next(self.vals_iter)\n",
    "        chosen_iter_params = dict(zip(self.iter_keys,v))\n",
    "        self.cfg_flat.update(chosen_iter_params)\n",
    "        return sample_config(unflatten(self.cfg_flat))\n",
    "    def __len__(self):\n",
    "        product = functools.partial(functools.reduce, operator.mul)\n",
    "        return product(len(v) for v in self.iter_vals) if self.vals else 1"
   ],
   "id": "6967d01ad7822088",
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:44.085018Z",
     "start_time": "2024-01-25T11:32:44.044772Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def vec_num2repr(val, base, prec, max_val):\n",
    "    \"\"\"\n",
    "    Convert numbers to a representation in a specified base with precision.\n",
    "\n",
    "    Parameters:\n",
    "    - val (np.array): The numbers to represent.\n",
    "    - base (int): The base of the representation.\n",
    "    - prec (int): The precision after the 'decimal' point in the base representation.\n",
    "    - max_val (float): The maximum absolute value of the number.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Sign and digits in the specified base representation.\n",
    "    \n",
    "    Examples:\n",
    "        With base=10, prec=2:\n",
    "            0.5   ->    50\n",
    "            3.52  ->   352\n",
    "            12.5  ->  1250\n",
    "    \"\"\"\n",
    "    base = float(base)\n",
    "    bs = val.shape[0]\n",
    "    sign = 1 * (val >= 0) - 1 * (val < 0)\n",
    "    val = np.abs(val)\n",
    "    max_bit_pos = int(np.ceil(np.log(max_val) / np.log(base)).item())\n",
    "\n",
    "    before_decimals = []\n",
    "    for i in range(max_bit_pos):\n",
    "        digit = (val / base**(max_bit_pos - i - 1)).astype(int)\n",
    "        before_decimals.append(digit)\n",
    "        val -= digit * base**(max_bit_pos - i - 1)\n",
    "\n",
    "    before_decimals = np.stack(before_decimals, axis=-1)\n",
    "\n",
    "    if prec > 0:\n",
    "        after_decimals = []\n",
    "        for i in range(prec):\n",
    "            digit = (val / base**(-i - 1)).astype(int)\n",
    "            after_decimals.append(digit)\n",
    "            val -= digit * base**(-i - 1)\n",
    "\n",
    "        after_decimals = np.stack(after_decimals, axis=-1)\n",
    "        digits = np.concatenate([before_decimals, after_decimals], axis=-1)\n",
    "    else:\n",
    "        digits = before_decimals\n",
    "    return sign, digits"
   ],
   "id": "b9bfe637e8723c82",
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:44.289713Z",
     "start_time": "2024-01-25T11:32:44.248243Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def vec_repr2num(sign, digits, base, prec, half_bin_correction=True):\n",
    "    \"\"\"\n",
    "    Convert a string representation in a specified base back to numbers.\n",
    "\n",
    "    Parameters:\n",
    "    - sign (np.array): The sign of the numbers.\n",
    "    - digits (np.array): Digits of the numbers in the specified base.\n",
    "    - base (int): The base of the representation.\n",
    "    - prec (int): The precision after the 'decimal' point in the base representation.\n",
    "    - half_bin_correction (bool): If True, adds 0.5 of the smallest bin size to the number.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Numbers corresponding to the given base representation.\n",
    "    \"\"\"\n",
    "    base = float(base)\n",
    "    bs, D = digits.shape\n",
    "    digits_flipped = np.flip(digits, axis=-1)\n",
    "    powers = -np.arange(-prec, -prec + D)\n",
    "    val = np.sum(digits_flipped/base**powers, axis=-1)\n",
    "\n",
    "    if half_bin_correction:\n",
    "        val += 0.5/base**prec\n",
    "\n",
    "    return sign * val"
   ],
   "id": "2a26b025a40fc777",
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:44.484812Z",
     "start_time": "2024-01-25T11:32:44.443279Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def serialize_arr(arr, settings: SerializerSettings):\n",
    "    \"\"\"\n",
    "    Serialize an array of numbers (a time series) into a string based on the provided settings.\n",
    "\n",
    "    Parameters:\n",
    "    - arr (np.array): Array of numbers to serialize.\n",
    "    - settings (SerializerSettings): Settings for serialization.\n",
    "\n",
    "    Returns:\n",
    "    - str: String representation of the array.\n",
    "    \"\"\"\n",
    "    # max_val is only for fixing the number of bits in nunm2repr so it can be vmapped\n",
    "    assert np.all(np.abs(arr[~np.isnan(arr)]) <= settings.max_val), f\"abs(arr) must be <= max_val,\\\n",
    "         but abs(arr)={np.abs(arr)}, max_val={settings.max_val}\"\n",
    "    \n",
    "    if not settings.signed:\n",
    "        assert np.all(arr[~np.isnan(arr)] >= 0), f\"unsigned arr must be >= 0\"\n",
    "        plus_sign = minus_sign = ''\n",
    "    else:\n",
    "        plus_sign = settings.plus_sign\n",
    "        minus_sign = settings.minus_sign\n",
    "    \n",
    "    vnum2repr = partial(vec_num2repr,base=settings.base,prec=settings.prec,max_val=settings.max_val)\n",
    "    sign_arr, digits_arr = vnum2repr(np.where(np.isnan(arr),np.zeros_like(arr),arr))\n",
    "    ismissing = np.isnan(arr)\n",
    "    \n",
    "    def tokenize(arr):\n",
    "        return ''.join([settings.bit_sep+str(b) for b in arr])\n",
    "    \n",
    "    bit_strs = []\n",
    "    for sign, digits,missing in zip(sign_arr, digits_arr, ismissing):\n",
    "        if not settings.fixed_length:\n",
    "            # remove leading zeros\n",
    "            nonzero_indices = np.where(digits != 0)[0]\n",
    "            if len(nonzero_indices) == 0:\n",
    "                digits = np.array([0])\n",
    "            else:\n",
    "                digits = digits[nonzero_indices[0]:]\n",
    "            # add a decimal point\n",
    "            prec = settings.prec\n",
    "            if len(settings.decimal_point):\n",
    "                digits = np.concatenate([digits[:-prec], np.array([settings.decimal_point]), digits[-prec:]])\n",
    "        digits = tokenize(digits)\n",
    "        sign_sep = plus_sign if sign==1 else minus_sign\n",
    "        if missing:\n",
    "            bit_strs.append(settings.missing_str)\n",
    "        else:\n",
    "            bit_strs.append(sign_sep + digits)\n",
    "    bit_str = settings.time_sep.join(bit_strs)\n",
    "    bit_str += settings.time_sep # otherwise there is ambiguity in number of digits in the last time step\n",
    "    return bit_str"
   ],
   "id": "834b79b5af727c90",
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:44.658418Z",
     "start_time": "2024-01-25T11:32:44.609528Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def deserialize_str(bit_str, settings: SerializerSettings, ignore_last=False, steps=None):\n",
    "    \"\"\"\n",
    "    Deserialize a string into an array of numbers (a time series) based on the provided settings.\n",
    "\n",
    "    Parameters:\n",
    "    - bit_str (str): String representation of an array of numbers.\n",
    "    - settings (SerializerSettings): Settings for deserialization.\n",
    "    - ignore_last (bool): If True, ignores the last time step in the string (which may be incomplete due to token limit etc.). Default is False.\n",
    "    - steps (int, optional): Number of steps or entries to deserialize.\n",
    "\n",
    "    Returns:\n",
    "    - None if deserialization failed for the very first number, otherwise \n",
    "    - np.array: Array of numbers corresponding to the string.\n",
    "    \"\"\"\n",
    "    # ignore_last is for ignoring the last time step in the prediction, which is often a partially generated due to token limit\n",
    "    orig_bitstring = bit_str\n",
    "    bit_strs = bit_str.split(settings.time_sep)\n",
    "    # remove empty strings\n",
    "    bit_strs = [a for a in bit_strs if len(a) > 0]\n",
    "    if ignore_last:\n",
    "        bit_strs = bit_strs[:-1]\n",
    "    if steps is not None:\n",
    "        bit_strs = bit_strs[:steps]\n",
    "    vrepr2num = partial(vec_repr2num,base=settings.base,prec=settings.prec,half_bin_correction=settings.half_bin_correction)\n",
    "    max_bit_pos = int(np.ceil(np.log(settings.max_val)/np.log(settings.base)).item())\n",
    "    sign_arr = []\n",
    "    digits_arr = []\n",
    "    try:\n",
    "        for i, bit_str in enumerate(bit_strs):\n",
    "            if bit_str.startswith(settings.minus_sign):\n",
    "                sign = -1\n",
    "            elif bit_str.startswith(settings.plus_sign):\n",
    "                sign = 1\n",
    "            else:\n",
    "                assert settings.signed == False, f\"signed bit_str must start with {settings.minus_sign} or {settings.plus_sign}\"\n",
    "            bit_str = bit_str[len(settings.plus_sign):] if sign==1 else bit_str[len(settings.minus_sign):]\n",
    "            if settings.bit_sep=='':\n",
    "                bits = [b for b in bit_str.lstrip()]\n",
    "            else:\n",
    "                bits = [b[:1] for b in bit_str.lstrip().split(settings.bit_sep)]\n",
    "            if settings.fixed_length:\n",
    "                assert len(bits) == max_bit_pos+settings.prec, f\"fixed length bit_str must have {max_bit_pos+settings.prec} bits, but has {len(bits)}: '{bit_str}'\"\n",
    "            digits = []\n",
    "            for b in bits:\n",
    "                if b==settings.decimal_point:\n",
    "                    continue\n",
    "                # check if is a digit\n",
    "                if b.isdigit():\n",
    "                    digits.append(int(b))\n",
    "                else:\n",
    "                    break\n",
    "            #digits = [int(b) for b in bits]\n",
    "            sign_arr.append(sign)\n",
    "            digits_arr.append(digits)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deserializing {settings.time_sep.join(bit_strs[i-2:i+5])}{settings.time_sep}\\n\\t{e}\")\n",
    "        print(f'Got {orig_bitstring}')\n",
    "        print(f\"Bitstr {bit_str}, separator {settings.bit_sep}\")\n",
    "        # At this point, we have already deserialized some of the bit_strs, so we return those below\n",
    "    if digits_arr:\n",
    "        # add leading zeros to get to equal lengths\n",
    "        max_len = max([len(d) for d in digits_arr])\n",
    "        for i in range(len(digits_arr)):\n",
    "            digits_arr[i] = [0]*(max_len-len(digits_arr[i])) + digits_arr[i]\n",
    "        return vrepr2num(np.array(sign_arr), np.array(digits_arr))\n",
    "    else:\n",
    "        # errored at first step\n",
    "        return None "
   ],
   "id": "3e5d422e922f0f58",
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:44.819463Z",
     "start_time": "2024-01-25T11:32:44.771522Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    \"\"\"\n",
    "    Represents a data scaler with transformation and inverse transformation functions.\n",
    "\n",
    "    Attributes:\n",
    "        transform (callable): Function to apply transformation.\n",
    "        inv_transform (callable): Function to apply inverse transformation.\n",
    "    \"\"\"\n",
    "    transform: callable = lambda x: x\n",
    "    inv_transform: callable = lambda x: x"
   ],
   "id": "461d56f45491718",
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:44.999634Z",
     "start_time": "2024-01-25T11:32:44.958925Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_scaler(history, alpha=0.95, beta=0.3, basic=False):\n",
    "    \"\"\"\n",
    "    Generate a Scaler object based on given history data.\n",
    "\n",
    "    Args:\n",
    "        history (array-like): Data to derive scaling from.\n",
    "        alpha (float, optional): Quantile for scaling. Defaults to .95.\n",
    "        # Truncate inputs\n",
    "        tokens = [tokeniz]\n",
    "        beta (float, optional): Shift parameter. Defaults to .3.\n",
    "        basic (bool, optional): If True, no shift is applied, and scaling by values below 0.01 is avoided. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Scaler: Configured scaler object.\n",
    "    \"\"\"\n",
    "    history = history[~np.isnan(history)]\n",
    "    if basic:\n",
    "        q = np.maximum(np.quantile(np.abs(history), alpha),.01)\n",
    "        def transform(x):\n",
    "            return x / q\n",
    "        def inv_transform(x):\n",
    "            return x * q\n",
    "    else:\n",
    "        min_ = np.min(history) - beta*(np.max(history)-np.min(history))\n",
    "        q = np.quantile(history-min_, alpha)\n",
    "        if q == 0:\n",
    "            q = 1\n",
    "        def transform(x):\n",
    "            return (x - min_) / q\n",
    "        def inv_transform(x):\n",
    "            return x * q + min_\n",
    "    return Scaler(transform=transform, inv_transform=inv_transform)"
   ],
   "id": "e0ec4863f8c7ab89",
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:45.191030Z",
     "start_time": "2024-01-25T11:32:45.148281Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def get_tokenizer(model):\n",
    "    name_parts = model.split(\"-\")\n",
    "    model_size = name_parts[0]\n",
    "    chat = len(name_parts) > 1\n",
    "    assert model_size in [\"7b\", \"13b\", \"70b\"]\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        llama2_model_string(model_size, chat),\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "    special_tokens_dict = dict()\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "    if tokenizer.unk_token is None:\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer"
   ],
   "id": "c5a77ad6c649731c",
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:45.364735Z",
     "start_time": "2024-01-25T11:32:45.328720Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenize_fn(str, model):\n",
    "    tokenizer = get_tokenizer(model)\n",
    "    return tokenizer(str)"
   ],
   "id": "6033e9e18e9a99fc",
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:45.555487Z",
     "start_time": "2024-01-25T11:32:45.511944Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenization_fns = {\n",
    "    'llama-7b': partial(tokenize_fn, model='7b'),\n",
    "    'llama-13b': partial(tokenize_fn, model='13b'),\n",
    "    'llama-70b': partial(tokenize_fn, model='70b'),\n",
    "    'llama-7b-chat': partial(tokenize_fn, model='7b-chat'),\n",
    "    'llama-13b-chat': partial(tokenize_fn, model='13b-chat'),\n",
    "    'llama-70b-chat': partial(tokenize_fn, model='70b-chat'),\n",
    "}\n",
    "\n",
    "# Optional: Context lengths for each model, only needed if you want automatic input truncation.\n",
    "context_lengths = {\n",
    "    'llama-7b': 4096,\n",
    "    'llama-13b': 4096,\n",
    "    'llama-70b': 4096,\n",
    "    'llama-7b-chat': 4096,\n",
    "    'llama-13b-chat': 4096,\n",
    "    'llama-70b-chat': 4096,\n",
    "}\n",
    "def truncate_input(input_arr, input_str, settings, steps):\n",
    "    \"\"\"\n",
    "    Truncate inputs to the maximum context length for a given model.\n",
    "    \"\"\"\n",
    "    tokenization_fn = partial(tokenize_fn, model='7b')\n",
    "    context_length = 4096\n",
    "    input_str_chuncks = input_str.split(settings.time_sep)\n",
    "    for i in range(len(input_str_chuncks) - 1):\n",
    "        truncated_input_str = settings.time_sep.join(input_str_chuncks[i:])\n",
    "        # add separator if not already present\n",
    "        if not truncated_input_str.endswith(settings.time_sep):\n",
    "            truncated_input_str += settings.time_sep\n",
    "        input_tokens = tokenization_fn(truncated_input_str)\n",
    "        num_input_tokens = len(input_tokens)\n",
    "        avg_token_length = num_input_tokens / (len(input_str_chuncks) - i)\n",
    "        STEP_MULTIPLIER = 1.2\n",
    "        num_output_tokens = avg_token_length * steps * STEP_MULTIPLIER\n",
    "        if num_input_tokens + num_output_tokens <= context_length:\n",
    "            truncated_input_arr = input_arr[i:]\n",
    "            break\n",
    "    if i > 0:\n",
    "        print(f'Warning: Truncated input from {len(input_arr)} to {len(truncated_input_arr)}')\n",
    "    return truncated_input_arr, truncated_input_str"
   ],
   "id": "77df46f32b0dab84",
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:45.715157Z",
     "start_time": "2024-01-25T11:32:45.680335Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def handle_prediction(pred, expected_length, strict=False):\n",
    "    \"\"\"\n",
    "    Process the output from LLM after deserialization, which may be too long or too short, or None if deserialization failed on the first prediction step.\n",
    "\n",
    "    Args:\n",
    "        pred (array-like or None): The predicted values. None indicates deserialization failed.\n",
    "        expected_length (int): Expected length of the prediction.\n",
    "        strict (bool, optional): If True, returns None for invalid predictions. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        array-like: Processed prediction.\n",
    "    \"\"\"\n",
    "    if pred is None:\n",
    "        return None\n",
    "    else:\n",
    "        if len(pred) < expected_length:\n",
    "            if strict:\n",
    "                print(f'Warning: Prediction too short {len(pred)} < {expected_length}, returning None')\n",
    "                return None\n",
    "            else:\n",
    "                print(f'Warning: Prediction too short {len(pred)} < {expected_length}, padded with last value')\n",
    "                return np.concatenate([pred, np.full(expected_length - len(pred), pred[-1])])\n",
    "        else:\n",
    "            return pred[:expected_length]"
   ],
   "id": "b1589f272761f325",
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:45.900487Z",
     "start_time": "2024-01-25T11:32:45.851872Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_predictions(\n",
    "    completion_fn,\n",
    "    input_strs,\n",
    "    steps,\n",
    "    settings: SerializerSettings,\n",
    "    scalers: None,\n",
    "    num_samples=1,\n",
    "    temp=0.7,\n",
    "    parallel=False,\n",
    "    strict_handling=False,\n",
    "    max_concurrent=10,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate and process text completions from a language model for input time series.\n",
    "\n",
    "    Args:\n",
    "        completion_fn (callable): Function to obtain text completions from the LLM.\n",
    "        input_strs (list of array-like): List of input time series.\n",
    "        steps (int): Number of steps to predict.\n",
    "        settings (SerializerSettings): Settings for serialization.\n",
    "        scalers (list of Scaler, optional): List of Scaler objects. Defaults to None, meaning no scaling is applied.\n",
    "        num_samples (int, optional): Number of samples to return. Defaults to 1.\n",
    "        temp (float, optional): Temperature for sampling. Defaults to 0.7.\n",
    "        parallel (bool, optional): If True, run completions in parallel. Defaults to True.\n",
    "        strict_handling (bool, optional): If True, return None for predictions that don't have exactly the right format or expected length. Defaults to False.\n",
    "        max_concurrent (int, optional): Maximum number of concurrent completions. Defaults to 50.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing:\n",
    "            - preds (list of lists): Numerical predictions.\n",
    "            - completions_list (list of lists): Raw text completions.\n",
    "            - input_strs (list of str): Serialized input strings.\n",
    "    \"\"\"\n",
    "\n",
    "    completions_list = []\n",
    "    STEP_MULTIPLIER = 1.2\n",
    "    complete = lambda x: completion_fn(input_str=x, steps=steps*STEP_MULTIPLIER, settings=settings, num_samples=num_samples, temp=temp)\n",
    "    if parallel and len(input_strs) > 1:\n",
    "        print('Running completions in parallel for each input')\n",
    "        with ThreadPoolExecutor(min(max_concurrent, len(input_strs))) as p:\n",
    "            completions_list = list(tqdm(p.map(complete, input_strs), total=len(input_strs)))\n",
    "    else:\n",
    "        completions_list = [complete(input_str) for input_str in tqdm(input_strs)]\n",
    "    def completion_to_pred(completion, inv_transform):\n",
    "        pred = handle_prediction(deserialize_str(completion, settings, ignore_last=False, steps=steps), expected_length=steps, strict=strict_handling)\n",
    "        if pred is not None:\n",
    "            return inv_transform(pred)\n",
    "        else:\n",
    "            return None\n",
    "    preds = [[completion_to_pred(completion, scaler.inv_transform) for completion in completions] for completions, scaler in zip(completions_list, scalers)]\n",
    "    return preds, completions_list, input_strs"
   ],
   "id": "aa7d0ced3d97ab04",
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:46.946036Z",
     "start_time": "2024-01-25T11:32:46.909949Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def llama2_model_string(model_size, chat):\n",
    "    chat = \"chat-\" if chat else \"\"\n",
    "    return f\"meta-llama/Llama-2-{model_size.lower()}-{chat}hf\""
   ],
   "id": "71fb4d2b8432f375",
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:48.010940Z",
     "start_time": "2024-01-25T11:32:47.288456Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LlamaForCausalLM, \n",
    "    LlamaTokenizer, \n",
    ")\n",
    "\n",
    "loaded = {}\n",
    "\n",
    "def get_model_and_tokenizer(model_name, cache_model=False):\n",
    "    if model_name in loaded:\n",
    "        return loaded[model_name]\n",
    "    name_parts = model_name.split(\"-\")\n",
    "    model_size = name_parts[0]\n",
    "    chat = len(name_parts) > 1\n",
    "\n",
    "    assert model_size in [\"7b\", \"13b\", \"70b\"]\n",
    "\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        llama2_model_string(model_size, chat),\n",
    "        device_map=\"auto\",   \n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.eval()\n",
    "    if cache_model:\n",
    "        loaded[model_name] = model, tokenizer\n",
    "    return model, tokenizer"
   ],
   "id": "158e708ff4af4c5b",
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:48.048810Z",
     "start_time": "2024-01-25T11:32:48.011579Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenize_fn(str, model):\n",
    "    tokenizer = get_tokenizer(model)\n",
    "    return tokenizer(str)"
   ],
   "id": "d9fce727581a593",
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:48.954594Z",
     "start_time": "2024-01-25T11:32:48.749137Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'jax' has no attribute 'version' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m grad,vmap\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mllama_nll_fn\u001B[39m(model, input_arr, target_arr, settings:SerializerSettings, transform, count_seps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, temp\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, cache_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[0;32m~/anaconda3/envs/llmtime/lib/python3.9/site-packages/jax/__init__.py:39\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _cloud_tpu_init\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Confusingly there are two things named \"config\": the module and the class.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# We want the exported object to be the class, so we first import the module\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# to make sure a later import doesn't overwrite the class.\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config \u001B[38;5;28;01mas\u001B[39;00m _config_module\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _config_module\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Force early import, allowing use of `jax.core` after importing `jax`.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/llmtime/lib/python3.9/site-packages/jax/config.py:15\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2018 The JAX Authors.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config \u001B[38;5;28;01mas\u001B[39;00m _deprecated_config  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Deprecations\u001B[39;00m\n\u001B[1;32m     19\u001B[0m _deprecations \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m# Added October 27, 2023\u001B[39;00m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m: (\n\u001B[1;32m     22\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccessing jax.config via the jax.config submodule is deprecated.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     23\u001B[0m         _deprecated_config),\n\u001B[1;32m     24\u001B[0m }\n",
      "File \u001B[0;32m~/anaconda3/envs/llmtime/lib/python3.9/site-packages/jax/_src/config.py:28\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Callable, Generic, NamedTuple, NoReturn, TypeVar\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lib\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jax_jit\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_src\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transfer_guard_lib\n",
      "File \u001B[0;32m~/anaconda3/envs/llmtime/lib/python3.9/site-packages/jax/_src/lib/__init__.py:75\u001B[0m\n\u001B[1;32m     70\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _jaxlib_version\n\u001B[1;32m     73\u001B[0m version_str \u001B[38;5;241m=\u001B[39m jaxlib\u001B[38;5;241m.\u001B[39mversion\u001B[38;5;241m.\u001B[39m__version__\n\u001B[1;32m     74\u001B[0m version \u001B[38;5;241m=\u001B[39m check_jaxlib_version(\n\u001B[0;32m---> 75\u001B[0m   jax_version\u001B[38;5;241m=\u001B[39m\u001B[43mjax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mversion\u001B[49m\u001B[38;5;241m.\u001B[39m__version__,\n\u001B[1;32m     76\u001B[0m   jaxlib_version\u001B[38;5;241m=\u001B[39mjaxlib\u001B[38;5;241m.\u001B[39mversion\u001B[38;5;241m.\u001B[39m__version__,\n\u001B[1;32m     77\u001B[0m   minimum_jaxlib_version\u001B[38;5;241m=\u001B[39mjax\u001B[38;5;241m.\u001B[39mversion\u001B[38;5;241m.\u001B[39m_minimum_jaxlib_version)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;66;03m# Before importing any C compiled modules from jaxlib, first import the CPU\u001B[39;00m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# feature guard module to verify that jaxlib was compiled in a way that only\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# uses instructions that are present on this machine.\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjaxlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcpu_feature_guard\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcpu_feature_guard\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: partially initialized module 'jax' has no attribute 'version' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from jax import grad,vmap\n",
    "import torch\n",
    "def llama_nll_fn(model, input_arr, target_arr, settings:SerializerSettings, transform, count_seps=True, temp=1, cache_model=True):\n",
    "    \"\"\" Returns the NLL/dimension (log base e) of the target array (continuous) according to the LM \n",
    "        conditioned on the input array. Applies relevant log determinant for transforms and\n",
    "        converts from discrete NLL of the LLM to continuous by assuming uniform within the bins.\n",
    "    inputs:\n",
    "        input_arr: (n,) context array\n",
    "        target_arr: (n,) ground truth array\n",
    "        cache_model: whether to cache the model and tokenizer for faster repeated calls\n",
    "    Returns: NLL/D\n",
    "    \"\"\"\n",
    "    model, tokenizer = get_model_and_tokenizer(model, cache_model=cache_model)\n",
    "\n",
    "    input_str = serialize_arr(vmap(transform)(input_arr), settings)\n",
    "    target_str = serialize_arr(vmap(transform)(target_arr), settings)\n",
    "    full_series = input_str + target_str\n",
    "    \n",
    "    batch = tokenizer(\n",
    "        [full_series], \n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    batch = {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch)\n",
    "\n",
    "    good_tokens_str = list(\"0123456789\" + settings.time_sep)\n",
    "    good_tokens = [tokenizer.convert_tokens_to_ids(token) for token in good_tokens_str]\n",
    "    bad_tokens = [i for i in range(len(tokenizer)) if i not in good_tokens]\n",
    "    out['logits'][:,:,bad_tokens] = -100\n",
    "\n",
    "    input_ids = batch['input_ids'][0][1:]\n",
    "    logprobs = torch.nn.functional.log_softmax(out['logits'], dim=-1)[0][:-1]\n",
    "    logprobs = logprobs[torch.arange(len(input_ids)), input_ids].cpu().numpy()\n",
    "\n",
    "    tokens = tokenizer.batch_decode(\n",
    "        input_ids,\n",
    "        skip_special_tokens=False, \n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    input_len = len(tokenizer([input_str], return_tensors=\"pt\",)['input_ids'][0])\n",
    "    input_len = input_len - 2 # remove the BOS token\n",
    "\n",
    "    logprobs = logprobs[input_len:]\n",
    "    tokens = tokens[input_len:]\n",
    "    BPD = -logprobs.sum()/len(target_arr)\n",
    "\n",
    "    #print(\"BPD unadjusted:\", -logprobs.sum()/len(target_arr), \"BPD adjusted:\", BPD)\n",
    "    # log p(x) = log p(token) - log bin_width = log p(token) + prec * log base\n",
    "    transformed_nll = BPD - settings.prec*np.log(settings.base)\n",
    "    avg_logdet_dydx = np.log(vmap(grad(transform))(target_arr)).mean()\n",
    "    return transformed_nll-avg_logdet_dydx"
   ],
   "id": "bd5c3cebd0ddcb61",
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:49.345462Z",
     "start_time": "2024-01-25T11:32:49.294654Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def llama_completion_fn(\n",
    "    model,\n",
    "    input_str,\n",
    "    steps,\n",
    "    settings,\n",
    "    batch_size=1,\n",
    "    num_samples=20,\n",
    "    temp=0.9, \n",
    "    top_p=0.9,\n",
    "    cache_model=True\n",
    "):\n",
    "    avg_tokens_per_step = len(tokenize_fn(input_str, model)['input_ids']) / len(input_str.split(settings.time_sep))\n",
    "    max_tokens = int(avg_tokens_per_step*steps)\n",
    "    \n",
    "    model, tokenizer = get_model_and_tokenizer(model, cache_model=cache_model)\n",
    "\n",
    "    gen_strs = []\n",
    "    for _ in tqdm(range(num_samples // batch_size)):\n",
    "        batch = tokenizer(\n",
    "            [input_str], \n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.repeat(batch_size, 1) for k, v in batch.items()}\n",
    "        print(batch)\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}\n",
    "        num_input_ids = batch['input_ids'].shape[1]\n",
    "\n",
    "        good_tokens_str = list(\"0123456789\" + settings.time_sep)\n",
    "        good_tokens = [tokenizer.convert_tokens_to_ids(token) for token in good_tokens_str]\n",
    "        # good_tokens += [tokenizer.eos_token_id]\n",
    "        bad_tokens = [i for i in range(len(tokenizer)) if i not in good_tokens]\n",
    "\n",
    "        generate_ids = model.generate(\n",
    "            **batch,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temp, \n",
    "            top_p=top_p, \n",
    "            bad_words_ids=[[t] for t in bad_tokens],\n",
    "            renormalize_logits=True,\n",
    "        )\n",
    "        gen_strs += tokenizer.batch_decode(\n",
    "            generate_ids[:, num_input_ids:],\n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False\n",
    "        )\n",
    "    return gen_strs"
   ],
   "id": "3173ca6ce5755a28",
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:32:55.758296Z",
     "start_time": "2024-01-25T11:32:55.711587Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_llmtime_predictions_data(train, test, model, settings, num_samples=10, temp=0.7, alpha=0.95, beta=0.3, basic=False, parallel=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Obtain forecasts from an LLM based on training series (history) and evaluate likelihood on test series (true future).\n",
    "    train and test can be either a single time series or a list of time series.\n",
    "    \"\"\"\n",
    "    completion_fn = llama_completion_fn\n",
    "    nll_fn = llama_nll_fn\n",
    "\n",
    "    if isinstance(settings, dict):\n",
    "        settings = SerializerSettings(**settings)\n",
    "    if not isinstance(train, list):\n",
    "        # Assume single train/test case\n",
    "        train = [train]\n",
    "        test = [test]\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        if not isinstance(train[i], pd.Series):\n",
    "            train[i] = pd.Series(train[i], index=pd.RangeIndex(len(train[i])))\n",
    "            test[i] = pd.Series(test[i], index=pd.RangeIndex(len(train[i]), len(test[i])+len(train[i])))\n",
    "            \n",
    "    print(train[0])\n",
    "    print(test[0])\n",
    "\n",
    "    test_len = len(test[0])\n",
    "    assert all(len(t)==test_len for t in test), f'All test series must have same length, got {[len(t) for t in test]}'\n",
    "\n",
    "    #Create a unique scaler for each series\n",
    "    scalers = [get_scaler(train[i].values, alpha=alpha, beta=beta, basic=basic) for i in range(len(train))]\n",
    "\n",
    "    # transform input_arrs\n",
    "    input_arrs = [train[i].values for i in range(len(train))]\n",
    "    transformed_input_arrs = np.array([scaler.transform(input_array) for input_array, scaler in zip(input_arrs, scalers)])\n",
    "    # serialize input_arrs\n",
    "    input_strs = [serialize_arr(scaled_input_arr, settings) for scaled_input_arr in transformed_input_arrs]\n",
    "    # Truncate input_arrs to fit the maximum context length\n",
    "    input_arrs, input_strs = zip(*[truncate_input(input_array, input_str, settings, model, test_len) for input_array, input_str in zip(input_arrs, input_strs)])\n",
    "\n",
    "    steps = test_len\n",
    "    samples = None\n",
    "    medians = None\n",
    "    completions_list = None\n",
    "    if num_samples > 0:\n",
    "        preds, completions_list, input_strs = generate_predictions(completion_fn, input_strs, steps, settings, scalers,\n",
    "                                                                    num_samples=num_samples, temp=temp,\n",
    "                                                                    parallel=parallel, **kwargs)\n",
    "        samples = [pd.DataFrame(preds[i], columns=test[i].index) for i in range(len(preds))]\n",
    "        medians = [sample.median(axis=0) for sample in samples]\n",
    "        samples = samples if len(samples) > 1 else samples[0]\n",
    "        medians = medians if len(medians) > 1 else medians[0]\n",
    "    out_dict = {\n",
    "        'samples': samples,\n",
    "        'median':  medians,\n",
    "        'info': {\n",
    "            'Method': model,\n",
    "        },\n",
    "        'completions_list': completions_list,\n",
    "        'input_strs': input_strs,\n",
    "    }\n",
    "    # Compute NLL/D on the true test series conditioned on the (truncated) input series\n",
    "    if nll_fn is not None:\n",
    "        BPDs = [nll_fn(input_arr=input_arrs[i], target_arr=test[i].values, settings=settings, transform=scalers[i].transform, count_seps=True, temp=temp) for i in range(len(train))]\n",
    "        out_dict['NLL/D'] = np.mean(BPDs)\n",
    "    return out_dict"
   ],
   "id": "2e29b8b3b254b8f5",
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:38:21.309276Z",
     "start_time": "2024-01-25T11:38:21.269443Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_autotuned_predictions_data(train, test, hypers, num_samples, get_predictions_fn, verbose=False, parallel=False, n_train=None, n_val=None):\n",
    "    \"\"\"\n",
    "    Automatically tunes hyperparameters based on validation likelihood and retrieves predictions using the best hyperparameters. The validation set is constructed on the fly by splitting the training set.\n",
    "    \"\"\"\n",
    "    if isinstance(hypers,dict):\n",
    "        hypers = list(grid_iter(hypers))\n",
    "    else:\n",
    "        assert isinstance(hypers, list), 'hypers must be a list or dict'\n",
    "    if not isinstance(train, list):\n",
    "        train = [train]\n",
    "        test = [test]\n",
    "    if n_val is None:\n",
    "        n_val = len(train)\n",
    "    print(hypers)\n",
    "    if len(hypers) > 1:\n",
    "        val_length = min(len(test[0]), int(np.mean([len(series) for series in train])/2))\n",
    "        train_minus_val, val, n_val = make_validation_dataset(train, n_val=n_val, val_length=val_length) # use half of train as val for tiny train sets\n",
    "        # remove validation series that has smaller length than required val_length\n",
    "        train_minus_val, val = zip(*[(train_series, val_series) for train_series, val_series in zip(train_minus_val, val) if len(val_series) == val_length])\n",
    "        train_minus_val = list(train_minus_val)\n",
    "        val = list(val)\n",
    "        print(f\"Tain - {train_minus_val}\")\n",
    "        print(f\"Val - {val}\")\n",
    "        if len(train_minus_val) <= int(0.9*n_val):\n",
    "            raise ValueError(f'Removed too many validation series. Only {len(train_minus_val)} out of {len(n_val)} series have length >= {val_length}. Try or decreasing val_length.')\n",
    "        val_nlls = []\n",
    "        def eval_hyper(hyper):\n",
    "            try:\n",
    "                return hyper, evaluate_hyper(hyper, train_minus_val, val, get_predictions_fn)\n",
    "            except ValueError:\n",
    "                return hyper, float('inf')\n",
    "\n",
    "        best_val_nll = float('inf')\n",
    "        best_hyper = None\n",
    "        if not parallel:\n",
    "            for hyper in tqdm(hypers, desc='Hyperparameter search'):\n",
    "                _,val_nll = eval_hyper(hyper)\n",
    "                val_nlls.append(val_nll)\n",
    "                if val_nll < best_val_nll:\n",
    "                    best_val_nll = val_nll\n",
    "                    best_hyper = hyper\n",
    "                if verbose:\n",
    "                    print(f'Hyper: {hyper} \\n\\t Val NLL: {val_nll:3f}')\n",
    "        else:\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                futures = [executor.submit(eval_hyper,hyper) for hyper in hypers]\n",
    "                for future in tqdm(as_completed(futures), total=len(hypers), desc='Hyperparameter search'):\n",
    "                    hyper,val_nll = future.result()\n",
    "                    val_nlls.append(val_nll)\n",
    "                    if val_nll < best_val_nll:\n",
    "                        best_val_nll = val_nll\n",
    "                        best_hyper = hyper\n",
    "                    if verbose:\n",
    "                        print(f'Hyper: {hyper} \\n\\t Val NLL: {val_nll:3f}')\n",
    "    else:\n",
    "        best_hyper = hypers[0]\n",
    "        best_val_nll = float('inf')\n",
    "    print(f'Sampling with best hyper... {best_hyper} \\n with NLL {best_val_nll:3f}')\n",
    "    out = get_predictions_fn(train, test, **best_hyper, num_samples=num_samples, n_train=n_train, parallel=parallel)\n",
    "    out['best_hyper']=convert_to_dict(best_hyper)\n",
    "    return out"
   ],
   "id": "1560630359a6577f",
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:38:21.573781Z",
     "start_time": "2024-01-25T11:38:21.530543Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def plot_preds(train, test, pred_dict, model_name, show_samples=True):\n",
    "    pred = pred_dict['median']\n",
    "    pred = pd.Series(pred, index=test.index)\n",
    "    plt.figure(figsize=(8, 6), dpi=100)\n",
    "    plt.plot(train)\n",
    "    plt.plot(test, label='Truth', color='black')\n",
    "    plt.plot(pred, label=model_name, color='purple')\n",
    "    # shade 90% confidence interval\n",
    "    samples = pred_dict['samples']\n",
    "    lower = np.quantile(samples, 0.05, axis=0)\n",
    "    upper = np.quantile(samples, 0.95, axis=0)\n",
    "    plt.fill_between(pred.index, lower, upper, alpha=0.3, color='purple')\n",
    "    if show_samples:\n",
    "        samples = pred_dict['samples']\n",
    "        # convert df to numpy array\n",
    "        samples = samples.values if isinstance(samples, pd.DataFrame) else samples\n",
    "        for i in range(min(10, samples.shape[0])):\n",
    "            plt.plot(pred.index, samples[i], color='purple', alpha=0.3, linewidth=1)\n",
    "    plt.legend(loc='upper left')\n",
    "    if 'NLL/D' in pred_dict:\n",
    "        nll = pred_dict['NLL/D']\n",
    "        if nll is not None:\n",
    "            plt.text(0.03, 0.85, f'NLL/D: {nll:.2f}', transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.5))\n",
    "    plt.show()"
   ],
   "id": "d14a27a01ade736d",
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:38:22.011682Z",
     "start_time": "2024-01-25T11:38:21.964761Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1557,) (668,) (2225,)\n"
     ]
    }
   ],
   "source": [
    "ds_name = 'CO2'\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "co2 = fetch_openml(data_id=41187, as_frame=True, parser='auto')\n",
    "co2_data = co2.frame\n",
    "co2_data[\"date\"] = pd.to_datetime(co2_data[[\"year\", \"month\", \"day\"]])\n",
    "co2_data = co2_data.sort_values(by=\"date\")\n",
    "co2_data = co2_data[[\"date\", \"co2\"]].set_index(\"date\")\n",
    "\n",
    "co2_data=co2_data.squeeze()\n",
    "train, test = co2_data[:int(0.7*len(co2_data))], co2_data[int(0.7*len(co2_data)):]\n",
    "print(train.shape,test.shape,co2_data.shape)"
   ],
   "id": "415748acff712bfb",
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T11:38:22.560177Z",
     "start_time": "2024-01-25T11:38:22.517325Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "out = {}\n",
    "hypers = list(grid_iter({'model': 'llama-7b', **llama2_hypers}))\n",
    "num_samples = 10\n",
    "pred_dict = get_autotuned_predictions_data(train, test, hypers, num_samples, get_llmtime_predictions_data, verbose=True)\n",
    "out[\"llama2\"] = pred_dict\n",
    "print(pred_dict)\n",
    "# plot_preds(train, test, pred_dict, \"llama2\", show_samples=True)"
   ],
   "id": "f387117137dc54d4",
   "execution_count": 81
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
