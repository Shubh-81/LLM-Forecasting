{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install outlines\n","!pip install transformers bitsandbytes>=0.39.0 -q\n","!pip install accelerate"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:07:25.247817Z","iopub.status.busy":"2024-04-02T09:07:25.247264Z","iopub.status.idle":"2024-04-02T09:07:26.918255Z","shell.execute_reply":"2024-04-02T09:07:26.917309Z","shell.execute_reply.started":"2024-04-02T09:07:25.247777Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login --token ''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from dataclasses import dataclass\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import seaborn as sns\n","from datetime import datetime\n","import pandas as pd\n","import numpy as np\n","import os\n","import torch\n","import outlines\n","from outlines import models\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from sklearn.datasets import fetch_openml\n","from sklearn.neighbors import KernelDensity\n","from transformers import BitsAndBytesConfig\n","\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:07:26.923684Z","iopub.status.busy":"2024-04-02T09:07:26.923356Z","iopub.status.idle":"2024-04-02T09:07:27.854643Z","shell.execute_reply":"2024-04-02T09:07:27.853748Z","shell.execute_reply.started":"2024-04-02T09:07:26.923648Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(1557,) (668,) (2225,)\n"]}],"source":["co2 = fetch_openml(data_id=41187, as_frame=True, parser='auto')\n","co2_data = co2.frame\n","co2_data[\"date\"] = pd.to_datetime(co2_data[[\"year\", \"month\", \"day\"]])\n","co2_data = co2_data.sort_values(by=\"date\")\n","co2_data = co2_data[[\"date\", \"co2\"]].set_index(\"date\")\n","\n","co2_data=co2_data.squeeze()\n","train, test = co2_data[:int(0.7*len(co2_data))], co2_data[int(0.7*len(co2_data)):]\n","print(train.shape,test.shape,co2_data.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:07:32.023392Z","iopub.status.busy":"2024-04-02T09:07:32.022945Z","iopub.status.idle":"2024-04-02T09:07:32.028571Z","shell.execute_reply":"2024-04-02T09:07:32.027432Z","shell.execute_reply.started":"2024-04-02T09:07:32.023365Z"},"trusted":true},"outputs":[],"source":["@dataclass\n","class Scaler:\n","    transform: callable = lambda x: x\n","    inv_transform: callable = lambda x: x"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:07:32.030556Z","iopub.status.busy":"2024-04-02T09:07:32.029882Z","iopub.status.idle":"2024-04-02T09:07:32.040004Z","shell.execute_reply":"2024-04-02T09:07:32.039083Z","shell.execute_reply.started":"2024-04-02T09:07:32.030524Z"},"trusted":true},"outputs":[],"source":["model_cache = {}"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:07:48.848679Z","iopub.status.busy":"2024-04-02T09:07:48.848317Z","iopub.status.idle":"2024-04-02T09:07:48.898177Z","shell.execute_reply":"2024-04-02T09:07:48.897271Z","shell.execute_reply.started":"2024-04-02T09:07:48.848651Z"},"trusted":true},"outputs":[],"source":["class LLMTime:\n","\n","    def __init__(self, train, test, alpha=0.95, beta=0.3, batch_length=400, basic=False, temperature=0.5, do_sample=True, model_name=\"mistralai/Mistral-7B-v0.1\", repetition_penalty=1.0, all_in_one=False, pretrain=False, load_in_4bit=True, num_samples=10):\n","        self.model_name = model_name\n","        self.train = train\n","        self.test = test\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.batch_length = batch_length\n","        self.basic = basic\n","        self.temperature = temperature\n","        self.do_sample = do_sample\n","        self.repetition_penalty = repetition_penalty\n","        self.scalers = None\n","        self.input_str = None\n","        self.test_str = None\n","        self.tokenizer = None\n","        self.model = None\n","        self.good_tokens = None\n","        self.bad_tokens = None\n","        self.output = None\n","        self.transformed_output_arr = None\n","        self.all_in_one = all_in_one\n","        self.pretrain = pretrain\n","        self.load_in_4bit = load_in_4bit\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.num_samples = num_samples\n","        self.preprocess_data()\n","\n","    @staticmethod\n","    def get_scaler(history, alpha=0.95, beta=0.3, basic=False):\n","        history = history[~np.isnan(history)]\n","        if basic:\n","            q = np.maximum(np.quantile(np.abs(history), alpha),.01)\n","            def transform(x):\n","                return x / q\n","            def inv_transform(x):\n","                return x * q\n","        else:\n","            min_ = np.min(history) - beta*(np.max(history)-np.min(history))\n","            q = np.quantile(history-min_, alpha)\n","            if q == 0:\n","                q = 1\n","            def transform(x):\n","                return (x - min_) / q\n","            def inv_transform(x):\n","                return x * q + min_\n","        return Scaler(transform=transform, inv_transform=inv_transform)\n","\n","    def convert_array_to_string(self, arr):\n","        rounded_values = [round(val * 1000) for val in arr]\n","        str_values = [str(val) for val in rounded_values]\n","        result_string = \",\".join(str_values)\n","        return result_string\n","\n","    def preprocess_data(self):\n","        train = self.train\n","        test = self.test\n","        if not isinstance(train, list):\n","            train = [train]\n","            test = [test]\n","        n_val = len(train)\n","        for i in range(len(train)):\n","            if not isinstance(train[i], pd.Series):\n","                train[i] = pd.Series(train[i], index=pd.RangeIndex(len(train[i])))\n","                test[i] = pd.Series(test[i], index=pd.RangeIndex(len(train[i]), len(test[i])+len(train[i])))\n","        alpha = self.alpha\n","        beta = self.beta\n","        basic = self.basic\n","        self.scalers = [self.get_scaler(train[i].values, alpha=alpha, beta=beta, basic=basic) for i in range(len(train))]\n","        input_arrs = [train[i].values for i in range(len(train))]\n","        transformed_input_arrs = np.array([scaler.transform(input_array) for input_array, scaler in zip(input_arrs, self.scalers)])\n","        input_str = self.convert_array_to_string(transformed_input_arrs[0])\n","        test_arrs = [test[i].values for i in range(len(test))]\n","        transformed_test_arrs = np.array([scaler.transform(input_array) for input_array, scaler in zip(test_arrs, self.scalers)])\n","        test_str = self.convert_array_to_string(transformed_test_arrs[0])\n","        self.input_str = input_str\n","        self.test_str = test_str\n","        return input_str, test_str, self.scalers\n","\n","    def zero_shot(self):\n","        input_str = self.input_str\n","        quantization_config = BitsAndBytesConfig(load_in_4bit=self.load_in_4bit)\n","        if model_cache.get(self.model_name) is not None:\n","            self.model = model_cache[self.model_name][0]\n","            self.tokenizer = model_cache[self.model_name][1]\n","        else:\n","            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map='auto', quantization_config=quantization_config)\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, padding_side=\"left\")\n","            model_cache[self.model_name] = [self.model, self.tokenizer]\n","        out = \"\"\n","        examples = []\n","        batch_length = self.batch_length\n","        for i in range(batch_length, len(input_str) - batch_length//5, batch_length):\n","            examples.append((input_str[i-batch_length:i], input_str[i:i+batch_length//5]))\n","            print((input_str[i-batch_length:i], input_str[i:i+batch_length//5]))\n","        print(examples)\n","        @outlines.prompt\n","        def labelling(to_label, examples):\n","            \"\"\"\n","            {% for example in examples %}\n","            {{ example[0] }} -> {{ example[1] }}\n","            {% endfor %}\n","            {{ to_label }} ->\n","            \"\"\"\n","        model = models.Transformers(self.model, self.tokenizer)\n","        generator = outlines.generate.regex(\n","            model,\n","            r\"[0-9,]{954}\",\n","        )\n","        print(len(self.test_str))\n","        k = 0\n","        while len(out) < len(self.test_str):\n","            print(k)\n","            k+=1\n","            prompt = labelling(input_str[-self.batch_length:], examples)\n","            answer = generator(prompt, max_tokens=(self.batch_length//5))\n","            input_str += answer\n","            out += answer\n","        return out\n","\n","    @staticmethod\n","    def invert_string_to_array(string_values):\n","        string_values = string_values.replace(\" \", \"\")\n","        if string_values[-1] == ',':\n","            string_values = string_values[:len(string_values) - 1]\n","        str_values_list = string_values.split(',')\n","        float_values = [float(val) if val.strip() != '' else 0.0 for val in str_values_list]\n","        original_values = [val / 1000 for val in float_values]\n","        return original_values\n","\n","    def get_original_array(self, output_str):\n","        output_arr = self.invert_string_to_array(string_values=output_str)\n","        output_arr = np.array(output_arr)\n","        transformed_output_arr = self.scalers[0].inv_transform(output_arr)\n","        self.transformed_output_arr = transformed_output_arr\n","        return transformed_output_arr\n","\n","    def plot(self, input, outputs, m):\n","          model_parameters = {\n","              \"alpha\": self.alpha,\n","              \"beta\": self.beta,\n","              \"batch_length\": self.batch_length,\n","              \"basic\": self.basic,\n","              \"temperature\": self.temperature,\n","              \"do_sample\": self.do_sample,\n","              \"repetition_penalty\": self.repetition_penalty,\n","              \"all_in_one\": self.all_in_one,\n","              \"pretrain\": self.pretrain,\n","              \"load_in_4bit\": self.load_in_4bit,\n","              \"num_samples\": self.num_samples\n","          }\n","          k = min(len(input), len(outputs[0]))\n","          sns.set(style=\"whitegrid\")\n","          fig, ax = plt.subplots(figsize=(10, 6))\n","          plt.title(f\"Few Shot Outlines {self.model_name}\")\n","          plt.axvline(x=m, color='k', linestyle='--', label='Train-Test Split')\n","          min_output = np.min(outputs, axis=0)[:k]\n","          max_output = np.max(outputs, axis=0)[:k]\n","          median_output = np.median(outputs, axis=0)[:k]\n","          mean_output = np.mean(outputs, axis=0)[:k]\n","          input = input[:k]\n","\n","\n","          for i in range(len(outputs)):\n","            plt.plot(outputs[i][:k], color='darkgray', alpha=0.5, label='_nolegend_')\n","\n","          plt.plot(min_output, color='green', linestyle='--', label='Min Predicted')\n","          plt.plot(max_output, color='orange', linestyle='--', label='Max Predicted')\n","          plt.plot(median_output, color='red', label='Median Predicted')\n","          plt.plot(mean_output, color='violet', label='Mean Predicted')\n","\n","          plt.plot(input, color='blue', label='Actual')\n","          plt.legend()\n","\n","          plt.grid(True)\n","          plt.xlabel('Time')\n","          sns.set_palette(\"husl\")\n","          \n","          min_input = np.min(input)\n","          max_input = np.max(input)\n","          plt.ylim(int(0.99*min_input), int(1.01*max_input))\n","\n","          mae = mean_absolute_error(input[-(len(input)-m):], median_output[-(len(median_output)-m):])\n","          rmse = np.sqrt(mean_squared_error(input[-(len(input)-m):], median_output[-(len(median_output)-m):]))\n","          input_range = np.max(input) - np.min(input)\n","          input_std = np.std(input)\n","          scaled_mae = mae / input_range\n","          scaled_rmse = rmse / input_std\n","          std_dev = np.std(outputs, axis=0)\n","          error_text = f\"\\n\\nMedian MAE: {mae:.2f}\\nMedian RMSE: {rmse:.2f}\\nStandard Deviation: {np.mean(std_dev):.2f}\"\n","          ax.text(1.05, 0.4, error_text, transform=ax.transAxes, fontsize=10, verticalalignment='center')\n","          textstr = '\\n'.join([f\"{key}: {value}\" for key, value in model_parameters.items()])\n","          props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n","          ax.text(1.05, 0.6, textstr, transform=ax.transAxes, fontsize=10, verticalalignment='center', bbox=props)\n","\n","          model_name = self.model_name.split('/')[1]\n","          current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","          random_number = np.random.randint(0, 1000)\n","          filename = f\"{model_name}-few-shot-outline-{current_time}-{random_number}.png\"\n","\n","          plt.savefig(filename, dpi=300, bbox_inches='tight')\n","          print(f\"Plot saved as {filename}\")\n","\n","    def run(self):\n","        outputs = []\n","        for i in range(self.num_samples):\n","            output = self.zero_shot()\n","            outputs.append(output)\n","        r = np.array(outputs)\n","        self.plot_and_return(r)\n","        return outputs\n","\n","    def plot_and_return(self, outputs):\n","      l = []\n","      for i, output in enumerate(outputs):\n","        firstCommaIndex = output.find(',')\n","        lastCommaIndex = output.rfind(',')\n","        outputs[i] = output[firstCommaIndex+1:lastCommaIndex]\n","      input_arr = self.get_original_array(self.input_str + ',' + self.test_str)\n","      inp = self.get_original_array(self.input_str)\n","      m = len(inp)\n","      min_len = 9999999999\n","      for i, output in enumerate(outputs):\n","        transformed_output_arr = self.get_original_array(self.input_str + ',' + output)\n","        min_len = min(min_len, len(transformed_output_arr))\n","        l.append(transformed_output_arr)\n","      input_arr = input_arr[:min_len]\n","      for i, x in enumerate(l):\n","        l[i] = x[:min_len]\n","      self.plot(input_arr, np.array(l), m)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["llm = LLMTime(train,\n","              test,\n","              alpha=1,\n","              beta=0.1,\n","              batch_length=1000,\n","              basic=False,\n","              temperature=0.7,\n","              do_sample=True,\n","              repetition_penalty=1,\n","              all_in_one=False,\n","              pretrain=False,\n","              load_in_4bit=True,\n","              num_samples=10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["llm.run()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
